This project leverages Large Language Models (LLMs) to automatically classify text as bullying or non-bullying. The model processes entire email/text content to capture contextual meaning rather than isolated keywords, enabling more accurate detection of harmful or abusive communication. The goal is to support safer digital spaces by applying modern NLP techniques for text classification.
