{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13061923,"sourceType":"datasetVersion","datasetId":8271540}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets torch accelerate peft\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-16T01:34:25.297771Z","iopub.execute_input":"2025-09-16T01:34:25.298377Z","iopub.status.idle":"2025-09-16T01:35:43.203063Z","shell.execute_reply.started":"2025-09-16T01:34:25.298357Z","shell.execute_reply":"2025-09-16T01:35:43.201996Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/cyber-bullying-new/Approach to Social Media Cyberbullying and Harassment Detection Using Advanced Machine Learning.csv\")\n\n# Show dataset info\nprint(\"Columns in dataset:\", df.columns.tolist())\nprint(\"\\nFirst 5 rows:\")\nprint(df.head())\n\n# If there is a column with messages/text, display a few random samples\nif 'text' in df.columns:\n    print(\"\\nSample messages:\")\n    print(df['text'].sample(5).to_list())\nelif 'message' in df.columns:\n    print(\"\\nSample messages:\")\n    print(df['message'].sample(5).to_list())\nelse:\n    print(\"\\nNo 'text' or 'message' column found. Check actual column names above.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T01:39:13.275225Z","iopub.execute_input":"2025-09-16T01:39:13.275564Z","iopub.status.idle":"2025-09-16T01:39:13.605347Z","shell.execute_reply.started":"2025-09-16T01:39:13.275531Z","shell.execute_reply":"2025-09-16T01:39:13.604669Z"}},"outputs":[{"name":"stdout","text":"Columns in dataset: ['Text', 'Label', 'Types']\n\nFirst 5 rows:\n                                                Text           Label  \\\n0  Ten outside soon doctor shake everyone treatme...    Not-Bullying   \n1  my life has come to a standstill and at this p...    Not-Bullying   \n2         girl this nigga make me sick to my stomach        Bullying   \n3                                   I wanna fuck you        Bullying   \n4  Oh hey, you should be ashamed of your disgusti...  Not - Bullying   \n\n       Types  \n0        NaN  \n1        NaN  \n2  Ethnicity  \n3     Sexual  \n4        NaN  \n\nNo 'text' or 'message' column found. Check actual column names above.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Keep only the two useful columns\ndf = df[['Text', 'Label']]\n\n# Clean and encode labels\ndf['Label'] = df['Label'].str.strip().str.lower()   # remove spaces + lowercase\ndf['label_id'] = df['Label'].map({'bullying': 1, 'not-bullying': 0})\n\n# Drop rows with missing values\ndf = df.dropna(subset=['Text', 'label_id'])\ndf['label_id'] = df['label_id'].astype(int)\n\n# Print dataset info\nprint(\"Unique labels:\", df['Label'].unique())\nprint(\"Label distribution:\\n\", df['label_id'].value_counts())\n\n# Split dataset (80/10/10)\ntrain_df, temp_df = train_test_split(\n    df, \n    test_size=0.2, \n    stratify=df['label_id'], \n    random_state=42\n)\ndev_df, test_df = train_test_split(\n    temp_df, \n    test_size=0.5, \n    stratify=temp_df['label_id'], \n    random_state=42\n)\n\nprint(f\"\\nTrain: {len(train_df)}, Dev: {len(dev_df)}, Test: {len(test_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T11:02:51.565260Z","iopub.execute_input":"2025-09-15T11:02:51.565478Z","iopub.status.idle":"2025-09-15T11:02:53.212299Z","shell.execute_reply.started":"2025-09-15T11:02:51.565452Z","shell.execute_reply":"2025-09-15T11:02:53.211491Z"}},"outputs":[{"name":"stdout","text":"Unique labels: ['not-bullying' 'bullying']\nLabel distribution:\n label_id\n1    4826\n0    3004\nName: count, dtype: int64\n\nTrain: 6264, Dev: 783, Test: 783\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ==========================\n# Model Development - DeBERTa V3 Small (Fast Debug Version)\n# ==========================\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, logging\n\n# ------------------------\n# 1. Load and preprocess\n# ------------------------\ndf = pd.read_csv(\"/kaggle/input/cyber-bullying-new/Approach to Social Media Cyberbullying and Harassment Detection Using Advanced Machine Learning.csv\")\n\n# Keep only needed columns\ndf = df[['Text', 'Label']]\ndf['Label'] = df['Label'].str.strip().str.lower()\n\n# Map to numeric labels\ndf['label'] = df['Label'].map({'bullying': 1, 'not-bullying': 0})\ndf = df.dropna(subset=['Text', 'label'])\ndf['label'] = df['label'].astype(int)\n\n# Split dataset (80/10/10 stratified)\ntrain_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\ndev_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n\nprint(f\"Train: {len(train_df)}, Dev: {len(dev_df)}, Test: {len(test_df)}\")\n\n# Convert to Hugging Face Dataset\ntrain_ds = Dataset.from_pandas(train_df)\ndev_ds = Dataset.from_pandas(dev_df)\ntest_ds = Dataset.from_pandas(test_df)\n\n# ------------------------\n# 2. Tokenization\n# ------------------------\nmodel_name = \"microsoft/deberta-v3-small\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef tokenize(batch):\n    return tokenizer(batch[\"Text\"], padding=\"max_length\", truncation=True, max_length=128)  # reduced max_length for speed\n\ntrain_ds = train_ds.map(tokenize, batched=True)\ndev_ds = dev_ds.map(tokenize, batched=True)\ntest_ds = test_ds.map(tokenize, batched=True)\n\ntrain_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\ndev_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\ntest_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\n# ------------------------\n# 3. Model\n# ------------------------\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n# ------------------------\n# 4. Metrics (scikit-learn)\n# ------------------------\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n    acc = accuracy_score(labels, preds)\n    return {\n        \"accuracy\": acc,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n    }\n\n# ------------------------\n# 5. Training\n# ------------------------\nlogging.set_verbosity_info()  # ensure logs are shown\n\ntraining_args = TrainingArguments(\n    output_dir=\"./deberta_results\",\n    do_eval=True,\n    per_device_train_batch_size=32,   # faster with bigger batch\n    per_device_eval_batch_size=32,\n    num_train_epochs=50,               # just 1 epoch for quick run\n    learning_rate=5e-5,               # slightly higher LR for faster learning\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=10,                 # frequent logging\n    save_total_limit=1,\n    report_to=\"none\",                 # no external trackers (wandb etc.)\n    disable_tqdm=False                # enables progress bar\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=dev_ds,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\n# Train the model\ntrainer.train()\n\n# ------------------------\n# 6. Evaluation\n# ------------------------\nmetrics = trainer.evaluate(test_ds)\nprint(\"Test metrics:\", metrics)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T01:40:02.437749Z","iopub.execute_input":"2025-09-16T01:40:02.438054Z","iopub.status.idle":"2025-09-16T02:23:18.826957Z","shell.execute_reply.started":"2025-09-16T01:40:02.438034Z","shell.execute_reply":"2025-09-16T02:23:18.826080Z"}},"outputs":[{"name":"stderr","text":"2025-09-16 01:40:19.097881: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757986819.325452      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757986819.393538      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Train: 6264, Dev: 783, Test: 783\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3227208d74c64009977a3a485ebf1c3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cfec946c49948d1a02db731c8d5d9d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00e23903bb5c4c2abfa2566df8eded8d"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6264 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f6574c4ddf947c1865199b715c32117"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/783 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d8fc5ff0b714b2e97650822edba8d2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/783 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab3a600be82f4accbcd910027f421d36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/286M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dd9b20292c6459ea2a3db118c97fac7"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\n/tmp/ipykernel_36/3481587281.py:93: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nSafetensors PR exists\nThe following columns in the Training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: Label, __index_level_0__, Text. If Label, __index_level_0__, Text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 6,264\n  Num Epochs = 50\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 9,800\n  Number of trainable parameters = 141,896,450\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/286M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae4e006e8bbd4e13bbbda8b2d7122000"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9800' max='9800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9800/9800 42:32, Epoch 50/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.678800</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.576300</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.419800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.364600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.345900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.247600</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.358500</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.318300</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.270100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.262700</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.293500</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.261200</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.189200</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.292100</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.332700</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.225100</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.247200</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.200300</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.241900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.155000</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.159000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.178300</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.236900</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.140900</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.218000</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.163400</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.208900</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.200600</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.172500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.155600</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.149000</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.157000</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.178200</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.135900</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.191000</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.211900</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.139400</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.177500</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.093800</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.076200</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.109100</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.098000</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.083900</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.070200</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.186700</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.175200</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.134700</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.096200</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.169200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.111800</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.173800</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.103200</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.094700</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.140300</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.144300</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.167800</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.157700</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.133100</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.101700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.066400</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.064600</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.108600</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.088800</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.101500</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.117900</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.121700</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.067700</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.096500</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.040500</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.075500</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.125400</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.096500</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.066500</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.102300</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.155500</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.164800</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.059700</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.115200</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.060800</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.075200</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.015000</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.092800</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.060300</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.068600</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.043400</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.070200</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.115200</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.083800</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.066000</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.083600</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.059400</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.085000</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.120600</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.077000</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.060500</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.143100</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.059000</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.071800</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.035600</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.089500</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.025200</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.094800</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.090500</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.041100</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.065700</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.037000</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.007200</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.100900</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.070500</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.024200</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.076900</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.053500</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.048900</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.080500</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.105900</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.070500</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.062600</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.035900</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.014400</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.019500</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.017000</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.031800</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.119200</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.049600</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.035200</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.045600</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.077600</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.059600</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.073400</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.047200</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.073300</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.034100</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.039300</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.057400</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.092700</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.043800</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.066400</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.008900</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.010300</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.062400</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.019900</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.018500</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.068700</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.038200</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.035000</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.034300</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.050600</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.034100</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.056500</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.084100</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.045500</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.058400</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.028200</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.057300</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.048400</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.039000</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.048800</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.049000</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.018700</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.037200</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.042800</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.037800</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.020900</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.022900</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.023700</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.030300</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.050600</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.063800</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.054400</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.036600</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.022900</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.081200</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.010600</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.026800</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.032300</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.032400</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.031200</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.028600</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.011100</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.040000</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.019200</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.033700</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.013200</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.057700</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.030800</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.047300</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.076000</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.041600</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.030000</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.027600</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.041300</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.025600</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.055200</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.030800</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.080900</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.057400</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.028300</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.020600</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.015200</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.009400</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>0.017800</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.028800</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>0.015700</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.049300</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.058700</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>0.062100</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>0.025100</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>0.005300</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>0.042800</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.022700</td>\n    </tr>\n    <tr>\n      <td>2110</td>\n      <td>0.036100</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>0.030000</td>\n    </tr>\n    <tr>\n      <td>2130</td>\n      <td>0.061600</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>0.017800</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.029200</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>0.036600</td>\n    </tr>\n    <tr>\n      <td>2170</td>\n      <td>0.029300</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>0.047700</td>\n    </tr>\n    <tr>\n      <td>2190</td>\n      <td>0.016100</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.032500</td>\n    </tr>\n    <tr>\n      <td>2210</td>\n      <td>0.038100</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>0.020100</td>\n    </tr>\n    <tr>\n      <td>2230</td>\n      <td>0.040400</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>0.067000</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.035200</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>0.017600</td>\n    </tr>\n    <tr>\n      <td>2270</td>\n      <td>0.022300</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>0.024900</td>\n    </tr>\n    <tr>\n      <td>2290</td>\n      <td>0.053100</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.033400</td>\n    </tr>\n    <tr>\n      <td>2310</td>\n      <td>0.032700</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>0.019900</td>\n    </tr>\n    <tr>\n      <td>2330</td>\n      <td>0.084100</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>0.055500</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.025400</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>0.019500</td>\n    </tr>\n    <tr>\n      <td>2370</td>\n      <td>0.016400</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>2390</td>\n      <td>0.026500</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.010600</td>\n    </tr>\n    <tr>\n      <td>2410</td>\n      <td>0.041600</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>0.026200</td>\n    </tr>\n    <tr>\n      <td>2430</td>\n      <td>0.013600</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>0.042100</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.014400</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>0.034400</td>\n    </tr>\n    <tr>\n      <td>2470</td>\n      <td>0.061300</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>0.051400</td>\n    </tr>\n    <tr>\n      <td>2490</td>\n      <td>0.022600</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.014100</td>\n    </tr>\n    <tr>\n      <td>2510</td>\n      <td>0.021000</td>\n    </tr>\n    <tr>\n      <td>2520</td>\n      <td>0.054500</td>\n    </tr>\n    <tr>\n      <td>2530</td>\n      <td>0.040500</td>\n    </tr>\n    <tr>\n      <td>2540</td>\n      <td>0.053600</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>0.011800</td>\n    </tr>\n    <tr>\n      <td>2560</td>\n      <td>0.031500</td>\n    </tr>\n    <tr>\n      <td>2570</td>\n      <td>0.036000</td>\n    </tr>\n    <tr>\n      <td>2580</td>\n      <td>0.019100</td>\n    </tr>\n    <tr>\n      <td>2590</td>\n      <td>0.027700</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.023000</td>\n    </tr>\n    <tr>\n      <td>2610</td>\n      <td>0.025500</td>\n    </tr>\n    <tr>\n      <td>2620</td>\n      <td>0.034200</td>\n    </tr>\n    <tr>\n      <td>2630</td>\n      <td>0.015000</td>\n    </tr>\n    <tr>\n      <td>2640</td>\n      <td>0.054200</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>0.019700</td>\n    </tr>\n    <tr>\n      <td>2660</td>\n      <td>0.014000</td>\n    </tr>\n    <tr>\n      <td>2670</td>\n      <td>0.039900</td>\n    </tr>\n    <tr>\n      <td>2680</td>\n      <td>0.017800</td>\n    </tr>\n    <tr>\n      <td>2690</td>\n      <td>0.025700</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.019800</td>\n    </tr>\n    <tr>\n      <td>2710</td>\n      <td>0.009800</td>\n    </tr>\n    <tr>\n      <td>2720</td>\n      <td>0.019100</td>\n    </tr>\n    <tr>\n      <td>2730</td>\n      <td>0.015400</td>\n    </tr>\n    <tr>\n      <td>2740</td>\n      <td>0.018000</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>0.026500</td>\n    </tr>\n    <tr>\n      <td>2760</td>\n      <td>0.017400</td>\n    </tr>\n    <tr>\n      <td>2770</td>\n      <td>0.022400</td>\n    </tr>\n    <tr>\n      <td>2780</td>\n      <td>0.017000</td>\n    </tr>\n    <tr>\n      <td>2790</td>\n      <td>0.066500</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.018100</td>\n    </tr>\n    <tr>\n      <td>2810</td>\n      <td>0.058900</td>\n    </tr>\n    <tr>\n      <td>2820</td>\n      <td>0.026300</td>\n    </tr>\n    <tr>\n      <td>2830</td>\n      <td>0.011700</td>\n    </tr>\n    <tr>\n      <td>2840</td>\n      <td>0.017600</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>0.016600</td>\n    </tr>\n    <tr>\n      <td>2860</td>\n      <td>0.048900</td>\n    </tr>\n    <tr>\n      <td>2870</td>\n      <td>0.010000</td>\n    </tr>\n    <tr>\n      <td>2880</td>\n      <td>0.020100</td>\n    </tr>\n    <tr>\n      <td>2890</td>\n      <td>0.024200</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.033900</td>\n    </tr>\n    <tr>\n      <td>2910</td>\n      <td>0.016900</td>\n    </tr>\n    <tr>\n      <td>2920</td>\n      <td>0.031900</td>\n    </tr>\n    <tr>\n      <td>2930</td>\n      <td>0.024000</td>\n    </tr>\n    <tr>\n      <td>2940</td>\n      <td>0.044900</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>0.026900</td>\n    </tr>\n    <tr>\n      <td>2960</td>\n      <td>0.023500</td>\n    </tr>\n    <tr>\n      <td>2970</td>\n      <td>0.016100</td>\n    </tr>\n    <tr>\n      <td>2980</td>\n      <td>0.035700</td>\n    </tr>\n    <tr>\n      <td>2990</td>\n      <td>0.020900</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.012500</td>\n    </tr>\n    <tr>\n      <td>3010</td>\n      <td>0.040500</td>\n    </tr>\n    <tr>\n      <td>3020</td>\n      <td>0.042400</td>\n    </tr>\n    <tr>\n      <td>3030</td>\n      <td>0.017400</td>\n    </tr>\n    <tr>\n      <td>3040</td>\n      <td>0.029200</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>0.021800</td>\n    </tr>\n    <tr>\n      <td>3060</td>\n      <td>0.018100</td>\n    </tr>\n    <tr>\n      <td>3070</td>\n      <td>0.016300</td>\n    </tr>\n    <tr>\n      <td>3080</td>\n      <td>0.011100</td>\n    </tr>\n    <tr>\n      <td>3090</td>\n      <td>0.026800</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.036300</td>\n    </tr>\n    <tr>\n      <td>3110</td>\n      <td>0.016800</td>\n    </tr>\n    <tr>\n      <td>3120</td>\n      <td>0.017400</td>\n    </tr>\n    <tr>\n      <td>3130</td>\n      <td>0.018900</td>\n    </tr>\n    <tr>\n      <td>3140</td>\n      <td>0.014000</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>0.021700</td>\n    </tr>\n    <tr>\n      <td>3160</td>\n      <td>0.027900</td>\n    </tr>\n    <tr>\n      <td>3170</td>\n      <td>0.011300</td>\n    </tr>\n    <tr>\n      <td>3180</td>\n      <td>0.034700</td>\n    </tr>\n    <tr>\n      <td>3190</td>\n      <td>0.014700</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.009100</td>\n    </tr>\n    <tr>\n      <td>3210</td>\n      <td>0.041000</td>\n    </tr>\n    <tr>\n      <td>3220</td>\n      <td>0.044900</td>\n    </tr>\n    <tr>\n      <td>3230</td>\n      <td>0.017600</td>\n    </tr>\n    <tr>\n      <td>3240</td>\n      <td>0.017700</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>0.040400</td>\n    </tr>\n    <tr>\n      <td>3260</td>\n      <td>0.021800</td>\n    </tr>\n    <tr>\n      <td>3270</td>\n      <td>0.016200</td>\n    </tr>\n    <tr>\n      <td>3280</td>\n      <td>0.049400</td>\n    </tr>\n    <tr>\n      <td>3290</td>\n      <td>0.083400</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.039800</td>\n    </tr>\n    <tr>\n      <td>3310</td>\n      <td>0.030700</td>\n    </tr>\n    <tr>\n      <td>3320</td>\n      <td>0.022300</td>\n    </tr>\n    <tr>\n      <td>3330</td>\n      <td>0.024300</td>\n    </tr>\n    <tr>\n      <td>3340</td>\n      <td>0.040700</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>0.027200</td>\n    </tr>\n    <tr>\n      <td>3360</td>\n      <td>0.018200</td>\n    </tr>\n    <tr>\n      <td>3370</td>\n      <td>0.037900</td>\n    </tr>\n    <tr>\n      <td>3380</td>\n      <td>0.021300</td>\n    </tr>\n    <tr>\n      <td>3390</td>\n      <td>0.038000</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.028600</td>\n    </tr>\n    <tr>\n      <td>3410</td>\n      <td>0.047800</td>\n    </tr>\n    <tr>\n      <td>3420</td>\n      <td>0.035800</td>\n    </tr>\n    <tr>\n      <td>3430</td>\n      <td>0.024900</td>\n    </tr>\n    <tr>\n      <td>3440</td>\n      <td>0.014000</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>0.038000</td>\n    </tr>\n    <tr>\n      <td>3460</td>\n      <td>0.024100</td>\n    </tr>\n    <tr>\n      <td>3470</td>\n      <td>0.017000</td>\n    </tr>\n    <tr>\n      <td>3480</td>\n      <td>0.023300</td>\n    </tr>\n    <tr>\n      <td>3490</td>\n      <td>0.025900</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.036100</td>\n    </tr>\n    <tr>\n      <td>3510</td>\n      <td>0.041700</td>\n    </tr>\n    <tr>\n      <td>3520</td>\n      <td>0.012600</td>\n    </tr>\n    <tr>\n      <td>3530</td>\n      <td>0.015600</td>\n    </tr>\n    <tr>\n      <td>3540</td>\n      <td>0.019500</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>0.020100</td>\n    </tr>\n    <tr>\n      <td>3560</td>\n      <td>0.012000</td>\n    </tr>\n    <tr>\n      <td>3570</td>\n      <td>0.017500</td>\n    </tr>\n    <tr>\n      <td>3580</td>\n      <td>0.014000</td>\n    </tr>\n    <tr>\n      <td>3590</td>\n      <td>0.043100</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.016400</td>\n    </tr>\n    <tr>\n      <td>3610</td>\n      <td>0.015200</td>\n    </tr>\n    <tr>\n      <td>3620</td>\n      <td>0.008200</td>\n    </tr>\n    <tr>\n      <td>3630</td>\n      <td>0.018500</td>\n    </tr>\n    <tr>\n      <td>3640</td>\n      <td>0.018100</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>0.076000</td>\n    </tr>\n    <tr>\n      <td>3660</td>\n      <td>0.050500</td>\n    </tr>\n    <tr>\n      <td>3670</td>\n      <td>0.020800</td>\n    </tr>\n    <tr>\n      <td>3680</td>\n      <td>0.041200</td>\n    </tr>\n    <tr>\n      <td>3690</td>\n      <td>0.027900</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.032100</td>\n    </tr>\n    <tr>\n      <td>3710</td>\n      <td>0.054200</td>\n    </tr>\n    <tr>\n      <td>3720</td>\n      <td>0.022000</td>\n    </tr>\n    <tr>\n      <td>3730</td>\n      <td>0.018800</td>\n    </tr>\n    <tr>\n      <td>3740</td>\n      <td>0.037400</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>0.030200</td>\n    </tr>\n    <tr>\n      <td>3760</td>\n      <td>0.013700</td>\n    </tr>\n    <tr>\n      <td>3770</td>\n      <td>0.031200</td>\n    </tr>\n    <tr>\n      <td>3780</td>\n      <td>0.022500</td>\n    </tr>\n    <tr>\n      <td>3790</td>\n      <td>0.014200</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.013200</td>\n    </tr>\n    <tr>\n      <td>3810</td>\n      <td>0.042000</td>\n    </tr>\n    <tr>\n      <td>3820</td>\n      <td>0.035600</td>\n    </tr>\n    <tr>\n      <td>3830</td>\n      <td>0.027400</td>\n    </tr>\n    <tr>\n      <td>3840</td>\n      <td>0.018400</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>0.039800</td>\n    </tr>\n    <tr>\n      <td>3860</td>\n      <td>0.012700</td>\n    </tr>\n    <tr>\n      <td>3870</td>\n      <td>0.022400</td>\n    </tr>\n    <tr>\n      <td>3880</td>\n      <td>0.025400</td>\n    </tr>\n    <tr>\n      <td>3890</td>\n      <td>0.023300</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.032200</td>\n    </tr>\n    <tr>\n      <td>3910</td>\n      <td>0.029600</td>\n    </tr>\n    <tr>\n      <td>3920</td>\n      <td>0.014700</td>\n    </tr>\n    <tr>\n      <td>3930</td>\n      <td>0.015100</td>\n    </tr>\n    <tr>\n      <td>3940</td>\n      <td>0.030600</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>0.010700</td>\n    </tr>\n    <tr>\n      <td>3960</td>\n      <td>0.025600</td>\n    </tr>\n    <tr>\n      <td>3970</td>\n      <td>0.010300</td>\n    </tr>\n    <tr>\n      <td>3980</td>\n      <td>0.041600</td>\n    </tr>\n    <tr>\n      <td>3990</td>\n      <td>0.010500</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.034800</td>\n    </tr>\n    <tr>\n      <td>4010</td>\n      <td>0.017700</td>\n    </tr>\n    <tr>\n      <td>4020</td>\n      <td>0.025500</td>\n    </tr>\n    <tr>\n      <td>4030</td>\n      <td>0.040400</td>\n    </tr>\n    <tr>\n      <td>4040</td>\n      <td>0.016100</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>0.022800</td>\n    </tr>\n    <tr>\n      <td>4060</td>\n      <td>0.028000</td>\n    </tr>\n    <tr>\n      <td>4070</td>\n      <td>0.041400</td>\n    </tr>\n    <tr>\n      <td>4080</td>\n      <td>0.010800</td>\n    </tr>\n    <tr>\n      <td>4090</td>\n      <td>0.030000</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.016500</td>\n    </tr>\n    <tr>\n      <td>4110</td>\n      <td>0.016800</td>\n    </tr>\n    <tr>\n      <td>4120</td>\n      <td>0.019500</td>\n    </tr>\n    <tr>\n      <td>4130</td>\n      <td>0.010300</td>\n    </tr>\n    <tr>\n      <td>4140</td>\n      <td>0.016100</td>\n    </tr>\n    <tr>\n      <td>4150</td>\n      <td>0.037600</td>\n    </tr>\n    <tr>\n      <td>4160</td>\n      <td>0.021700</td>\n    </tr>\n    <tr>\n      <td>4170</td>\n      <td>0.021000</td>\n    </tr>\n    <tr>\n      <td>4180</td>\n      <td>0.020300</td>\n    </tr>\n    <tr>\n      <td>4190</td>\n      <td>0.045000</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.019700</td>\n    </tr>\n    <tr>\n      <td>4210</td>\n      <td>0.026000</td>\n    </tr>\n    <tr>\n      <td>4220</td>\n      <td>0.019500</td>\n    </tr>\n    <tr>\n      <td>4230</td>\n      <td>0.056200</td>\n    </tr>\n    <tr>\n      <td>4240</td>\n      <td>0.020400</td>\n    </tr>\n    <tr>\n      <td>4250</td>\n      <td>0.021600</td>\n    </tr>\n    <tr>\n      <td>4260</td>\n      <td>0.028200</td>\n    </tr>\n    <tr>\n      <td>4270</td>\n      <td>0.018600</td>\n    </tr>\n    <tr>\n      <td>4280</td>\n      <td>0.028000</td>\n    </tr>\n    <tr>\n      <td>4290</td>\n      <td>0.027600</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.018800</td>\n    </tr>\n    <tr>\n      <td>4310</td>\n      <td>0.026500</td>\n    </tr>\n    <tr>\n      <td>4320</td>\n      <td>0.014900</td>\n    </tr>\n    <tr>\n      <td>4330</td>\n      <td>0.007500</td>\n    </tr>\n    <tr>\n      <td>4340</td>\n      <td>0.032100</td>\n    </tr>\n    <tr>\n      <td>4350</td>\n      <td>0.037300</td>\n    </tr>\n    <tr>\n      <td>4360</td>\n      <td>0.020000</td>\n    </tr>\n    <tr>\n      <td>4370</td>\n      <td>0.027600</td>\n    </tr>\n    <tr>\n      <td>4380</td>\n      <td>0.033700</td>\n    </tr>\n    <tr>\n      <td>4390</td>\n      <td>0.010100</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.015800</td>\n    </tr>\n    <tr>\n      <td>4410</td>\n      <td>0.058500</td>\n    </tr>\n    <tr>\n      <td>4420</td>\n      <td>0.036000</td>\n    </tr>\n    <tr>\n      <td>4430</td>\n      <td>0.031900</td>\n    </tr>\n    <tr>\n      <td>4440</td>\n      <td>0.019100</td>\n    </tr>\n    <tr>\n      <td>4450</td>\n      <td>0.010100</td>\n    </tr>\n    <tr>\n      <td>4460</td>\n      <td>0.016500</td>\n    </tr>\n    <tr>\n      <td>4470</td>\n      <td>0.029800</td>\n    </tr>\n    <tr>\n      <td>4480</td>\n      <td>0.028900</td>\n    </tr>\n    <tr>\n      <td>4490</td>\n      <td>0.021100</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.024600</td>\n    </tr>\n    <tr>\n      <td>4510</td>\n      <td>0.024700</td>\n    </tr>\n    <tr>\n      <td>4520</td>\n      <td>0.023000</td>\n    </tr>\n    <tr>\n      <td>4530</td>\n      <td>0.014500</td>\n    </tr>\n    <tr>\n      <td>4540</td>\n      <td>0.077200</td>\n    </tr>\n    <tr>\n      <td>4550</td>\n      <td>0.033100</td>\n    </tr>\n    <tr>\n      <td>4560</td>\n      <td>0.026600</td>\n    </tr>\n    <tr>\n      <td>4570</td>\n      <td>0.025700</td>\n    </tr>\n    <tr>\n      <td>4580</td>\n      <td>0.011100</td>\n    </tr>\n    <tr>\n      <td>4590</td>\n      <td>0.019600</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.038100</td>\n    </tr>\n    <tr>\n      <td>4610</td>\n      <td>0.027600</td>\n    </tr>\n    <tr>\n      <td>4620</td>\n      <td>0.031400</td>\n    </tr>\n    <tr>\n      <td>4630</td>\n      <td>0.036800</td>\n    </tr>\n    <tr>\n      <td>4640</td>\n      <td>0.040300</td>\n    </tr>\n    <tr>\n      <td>4650</td>\n      <td>0.008900</td>\n    </tr>\n    <tr>\n      <td>4660</td>\n      <td>0.077200</td>\n    </tr>\n    <tr>\n      <td>4670</td>\n      <td>0.016100</td>\n    </tr>\n    <tr>\n      <td>4680</td>\n      <td>0.034400</td>\n    </tr>\n    <tr>\n      <td>4690</td>\n      <td>0.021800</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.046000</td>\n    </tr>\n    <tr>\n      <td>4710</td>\n      <td>0.021500</td>\n    </tr>\n    <tr>\n      <td>4720</td>\n      <td>0.014400</td>\n    </tr>\n    <tr>\n      <td>4730</td>\n      <td>0.035800</td>\n    </tr>\n    <tr>\n      <td>4740</td>\n      <td>0.013200</td>\n    </tr>\n    <tr>\n      <td>4750</td>\n      <td>0.017800</td>\n    </tr>\n    <tr>\n      <td>4760</td>\n      <td>0.027200</td>\n    </tr>\n    <tr>\n      <td>4770</td>\n      <td>0.028800</td>\n    </tr>\n    <tr>\n      <td>4780</td>\n      <td>0.034500</td>\n    </tr>\n    <tr>\n      <td>4790</td>\n      <td>0.026200</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.015300</td>\n    </tr>\n    <tr>\n      <td>4810</td>\n      <td>0.029800</td>\n    </tr>\n    <tr>\n      <td>4820</td>\n      <td>0.005400</td>\n    </tr>\n    <tr>\n      <td>4830</td>\n      <td>0.028100</td>\n    </tr>\n    <tr>\n      <td>4840</td>\n      <td>0.022500</td>\n    </tr>\n    <tr>\n      <td>4850</td>\n      <td>0.011100</td>\n    </tr>\n    <tr>\n      <td>4860</td>\n      <td>0.020700</td>\n    </tr>\n    <tr>\n      <td>4870</td>\n      <td>0.039200</td>\n    </tr>\n    <tr>\n      <td>4880</td>\n      <td>0.043900</td>\n    </tr>\n    <tr>\n      <td>4890</td>\n      <td>0.024000</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.022200</td>\n    </tr>\n    <tr>\n      <td>4910</td>\n      <td>0.009300</td>\n    </tr>\n    <tr>\n      <td>4920</td>\n      <td>0.016500</td>\n    </tr>\n    <tr>\n      <td>4930</td>\n      <td>0.025800</td>\n    </tr>\n    <tr>\n      <td>4940</td>\n      <td>0.022000</td>\n    </tr>\n    <tr>\n      <td>4950</td>\n      <td>0.019900</td>\n    </tr>\n    <tr>\n      <td>4960</td>\n      <td>0.025200</td>\n    </tr>\n    <tr>\n      <td>4970</td>\n      <td>0.022400</td>\n    </tr>\n    <tr>\n      <td>4980</td>\n      <td>0.016700</td>\n    </tr>\n    <tr>\n      <td>4990</td>\n      <td>0.027300</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.014300</td>\n    </tr>\n    <tr>\n      <td>5010</td>\n      <td>0.021200</td>\n    </tr>\n    <tr>\n      <td>5020</td>\n      <td>0.021600</td>\n    </tr>\n    <tr>\n      <td>5030</td>\n      <td>0.030100</td>\n    </tr>\n    <tr>\n      <td>5040</td>\n      <td>0.011500</td>\n    </tr>\n    <tr>\n      <td>5050</td>\n      <td>0.029400</td>\n    </tr>\n    <tr>\n      <td>5060</td>\n      <td>0.020700</td>\n    </tr>\n    <tr>\n      <td>5070</td>\n      <td>0.022900</td>\n    </tr>\n    <tr>\n      <td>5080</td>\n      <td>0.023500</td>\n    </tr>\n    <tr>\n      <td>5090</td>\n      <td>0.021300</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.023300</td>\n    </tr>\n    <tr>\n      <td>5110</td>\n      <td>0.032900</td>\n    </tr>\n    <tr>\n      <td>5120</td>\n      <td>0.011000</td>\n    </tr>\n    <tr>\n      <td>5130</td>\n      <td>0.030300</td>\n    </tr>\n    <tr>\n      <td>5140</td>\n      <td>0.008200</td>\n    </tr>\n    <tr>\n      <td>5150</td>\n      <td>0.023600</td>\n    </tr>\n    <tr>\n      <td>5160</td>\n      <td>0.018600</td>\n    </tr>\n    <tr>\n      <td>5170</td>\n      <td>0.025900</td>\n    </tr>\n    <tr>\n      <td>5180</td>\n      <td>0.015300</td>\n    </tr>\n    <tr>\n      <td>5190</td>\n      <td>0.015900</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.013700</td>\n    </tr>\n    <tr>\n      <td>5210</td>\n      <td>0.016300</td>\n    </tr>\n    <tr>\n      <td>5220</td>\n      <td>0.013900</td>\n    </tr>\n    <tr>\n      <td>5230</td>\n      <td>0.019000</td>\n    </tr>\n    <tr>\n      <td>5240</td>\n      <td>0.037700</td>\n    </tr>\n    <tr>\n      <td>5250</td>\n      <td>0.017700</td>\n    </tr>\n    <tr>\n      <td>5260</td>\n      <td>0.013300</td>\n    </tr>\n    <tr>\n      <td>5270</td>\n      <td>0.030900</td>\n    </tr>\n    <tr>\n      <td>5280</td>\n      <td>0.029300</td>\n    </tr>\n    <tr>\n      <td>5290</td>\n      <td>0.023500</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.018900</td>\n    </tr>\n    <tr>\n      <td>5310</td>\n      <td>0.012000</td>\n    </tr>\n    <tr>\n      <td>5320</td>\n      <td>0.019000</td>\n    </tr>\n    <tr>\n      <td>5330</td>\n      <td>0.012400</td>\n    </tr>\n    <tr>\n      <td>5340</td>\n      <td>0.020400</td>\n    </tr>\n    <tr>\n      <td>5350</td>\n      <td>0.025000</td>\n    </tr>\n    <tr>\n      <td>5360</td>\n      <td>0.018900</td>\n    </tr>\n    <tr>\n      <td>5370</td>\n      <td>0.017200</td>\n    </tr>\n    <tr>\n      <td>5380</td>\n      <td>0.012000</td>\n    </tr>\n    <tr>\n      <td>5390</td>\n      <td>0.017800</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.036500</td>\n    </tr>\n    <tr>\n      <td>5410</td>\n      <td>0.013800</td>\n    </tr>\n    <tr>\n      <td>5420</td>\n      <td>0.018100</td>\n    </tr>\n    <tr>\n      <td>5430</td>\n      <td>0.024100</td>\n    </tr>\n    <tr>\n      <td>5440</td>\n      <td>0.024600</td>\n    </tr>\n    <tr>\n      <td>5450</td>\n      <td>0.025500</td>\n    </tr>\n    <tr>\n      <td>5460</td>\n      <td>0.026500</td>\n    </tr>\n    <tr>\n      <td>5470</td>\n      <td>0.032000</td>\n    </tr>\n    <tr>\n      <td>5480</td>\n      <td>0.007700</td>\n    </tr>\n    <tr>\n      <td>5490</td>\n      <td>0.035600</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.016800</td>\n    </tr>\n    <tr>\n      <td>5510</td>\n      <td>0.023900</td>\n    </tr>\n    <tr>\n      <td>5520</td>\n      <td>0.021700</td>\n    </tr>\n    <tr>\n      <td>5530</td>\n      <td>0.024400</td>\n    </tr>\n    <tr>\n      <td>5540</td>\n      <td>0.031700</td>\n    </tr>\n    <tr>\n      <td>5550</td>\n      <td>0.015000</td>\n    </tr>\n    <tr>\n      <td>5560</td>\n      <td>0.024400</td>\n    </tr>\n    <tr>\n      <td>5570</td>\n      <td>0.014500</td>\n    </tr>\n    <tr>\n      <td>5580</td>\n      <td>0.019400</td>\n    </tr>\n    <tr>\n      <td>5590</td>\n      <td>0.017200</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.023000</td>\n    </tr>\n    <tr>\n      <td>5610</td>\n      <td>0.012600</td>\n    </tr>\n    <tr>\n      <td>5620</td>\n      <td>0.015200</td>\n    </tr>\n    <tr>\n      <td>5630</td>\n      <td>0.011600</td>\n    </tr>\n    <tr>\n      <td>5640</td>\n      <td>0.023000</td>\n    </tr>\n    <tr>\n      <td>5650</td>\n      <td>0.019800</td>\n    </tr>\n    <tr>\n      <td>5660</td>\n      <td>0.023000</td>\n    </tr>\n    <tr>\n      <td>5670</td>\n      <td>0.025900</td>\n    </tr>\n    <tr>\n      <td>5680</td>\n      <td>0.023300</td>\n    </tr>\n    <tr>\n      <td>5690</td>\n      <td>0.033700</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>0.020800</td>\n    </tr>\n    <tr>\n      <td>5710</td>\n      <td>0.011800</td>\n    </tr>\n    <tr>\n      <td>5720</td>\n      <td>0.018800</td>\n    </tr>\n    <tr>\n      <td>5730</td>\n      <td>0.017400</td>\n    </tr>\n    <tr>\n      <td>5740</td>\n      <td>0.016300</td>\n    </tr>\n    <tr>\n      <td>5750</td>\n      <td>0.022900</td>\n    </tr>\n    <tr>\n      <td>5760</td>\n      <td>0.024300</td>\n    </tr>\n    <tr>\n      <td>5770</td>\n      <td>0.015700</td>\n    </tr>\n    <tr>\n      <td>5780</td>\n      <td>0.015600</td>\n    </tr>\n    <tr>\n      <td>5790</td>\n      <td>0.027200</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>0.029400</td>\n    </tr>\n    <tr>\n      <td>5810</td>\n      <td>0.022400</td>\n    </tr>\n    <tr>\n      <td>5820</td>\n      <td>0.021000</td>\n    </tr>\n    <tr>\n      <td>5830</td>\n      <td>0.009500</td>\n    </tr>\n    <tr>\n      <td>5840</td>\n      <td>0.017200</td>\n    </tr>\n    <tr>\n      <td>5850</td>\n      <td>0.017600</td>\n    </tr>\n    <tr>\n      <td>5860</td>\n      <td>0.024800</td>\n    </tr>\n    <tr>\n      <td>5870</td>\n      <td>0.016000</td>\n    </tr>\n    <tr>\n      <td>5880</td>\n      <td>0.026500</td>\n    </tr>\n    <tr>\n      <td>5890</td>\n      <td>0.018600</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>0.048000</td>\n    </tr>\n    <tr>\n      <td>5910</td>\n      <td>0.019200</td>\n    </tr>\n    <tr>\n      <td>5920</td>\n      <td>0.004900</td>\n    </tr>\n    <tr>\n      <td>5930</td>\n      <td>0.016100</td>\n    </tr>\n    <tr>\n      <td>5940</td>\n      <td>0.027300</td>\n    </tr>\n    <tr>\n      <td>5950</td>\n      <td>0.033600</td>\n    </tr>\n    <tr>\n      <td>5960</td>\n      <td>0.013800</td>\n    </tr>\n    <tr>\n      <td>5970</td>\n      <td>0.013600</td>\n    </tr>\n    <tr>\n      <td>5980</td>\n      <td>0.012200</td>\n    </tr>\n    <tr>\n      <td>5990</td>\n      <td>0.039100</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.015600</td>\n    </tr>\n    <tr>\n      <td>6010</td>\n      <td>0.018500</td>\n    </tr>\n    <tr>\n      <td>6020</td>\n      <td>0.023900</td>\n    </tr>\n    <tr>\n      <td>6030</td>\n      <td>0.020300</td>\n    </tr>\n    <tr>\n      <td>6040</td>\n      <td>0.019500</td>\n    </tr>\n    <tr>\n      <td>6050</td>\n      <td>0.020800</td>\n    </tr>\n    <tr>\n      <td>6060</td>\n      <td>0.026200</td>\n    </tr>\n    <tr>\n      <td>6070</td>\n      <td>0.017100</td>\n    </tr>\n    <tr>\n      <td>6080</td>\n      <td>0.045400</td>\n    </tr>\n    <tr>\n      <td>6090</td>\n      <td>0.040200</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>0.027200</td>\n    </tr>\n    <tr>\n      <td>6110</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>6120</td>\n      <td>0.011600</td>\n    </tr>\n    <tr>\n      <td>6130</td>\n      <td>0.023800</td>\n    </tr>\n    <tr>\n      <td>6140</td>\n      <td>0.010500</td>\n    </tr>\n    <tr>\n      <td>6150</td>\n      <td>0.063800</td>\n    </tr>\n    <tr>\n      <td>6160</td>\n      <td>0.019900</td>\n    </tr>\n    <tr>\n      <td>6170</td>\n      <td>0.031400</td>\n    </tr>\n    <tr>\n      <td>6180</td>\n      <td>0.026500</td>\n    </tr>\n    <tr>\n      <td>6190</td>\n      <td>0.010700</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>0.027700</td>\n    </tr>\n    <tr>\n      <td>6210</td>\n      <td>0.013100</td>\n    </tr>\n    <tr>\n      <td>6220</td>\n      <td>0.012500</td>\n    </tr>\n    <tr>\n      <td>6230</td>\n      <td>0.045600</td>\n    </tr>\n    <tr>\n      <td>6240</td>\n      <td>0.022100</td>\n    </tr>\n    <tr>\n      <td>6250</td>\n      <td>0.021700</td>\n    </tr>\n    <tr>\n      <td>6260</td>\n      <td>0.017400</td>\n    </tr>\n    <tr>\n      <td>6270</td>\n      <td>0.020100</td>\n    </tr>\n    <tr>\n      <td>6280</td>\n      <td>0.014200</td>\n    </tr>\n    <tr>\n      <td>6290</td>\n      <td>0.021700</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>0.023200</td>\n    </tr>\n    <tr>\n      <td>6310</td>\n      <td>0.021600</td>\n    </tr>\n    <tr>\n      <td>6320</td>\n      <td>0.035600</td>\n    </tr>\n    <tr>\n      <td>6330</td>\n      <td>0.020200</td>\n    </tr>\n    <tr>\n      <td>6340</td>\n      <td>0.016500</td>\n    </tr>\n    <tr>\n      <td>6350</td>\n      <td>0.023800</td>\n    </tr>\n    <tr>\n      <td>6360</td>\n      <td>0.013200</td>\n    </tr>\n    <tr>\n      <td>6370</td>\n      <td>0.018800</td>\n    </tr>\n    <tr>\n      <td>6380</td>\n      <td>0.012000</td>\n    </tr>\n    <tr>\n      <td>6390</td>\n      <td>0.029900</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>0.023500</td>\n    </tr>\n    <tr>\n      <td>6410</td>\n      <td>0.014500</td>\n    </tr>\n    <tr>\n      <td>6420</td>\n      <td>0.014400</td>\n    </tr>\n    <tr>\n      <td>6430</td>\n      <td>0.020500</td>\n    </tr>\n    <tr>\n      <td>6440</td>\n      <td>0.038800</td>\n    </tr>\n    <tr>\n      <td>6450</td>\n      <td>0.017700</td>\n    </tr>\n    <tr>\n      <td>6460</td>\n      <td>0.021400</td>\n    </tr>\n    <tr>\n      <td>6470</td>\n      <td>0.017600</td>\n    </tr>\n    <tr>\n      <td>6480</td>\n      <td>0.013100</td>\n    </tr>\n    <tr>\n      <td>6490</td>\n      <td>0.026500</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.013500</td>\n    </tr>\n    <tr>\n      <td>6510</td>\n      <td>0.017300</td>\n    </tr>\n    <tr>\n      <td>6520</td>\n      <td>0.020100</td>\n    </tr>\n    <tr>\n      <td>6530</td>\n      <td>0.014700</td>\n    </tr>\n    <tr>\n      <td>6540</td>\n      <td>0.018000</td>\n    </tr>\n    <tr>\n      <td>6550</td>\n      <td>0.008600</td>\n    </tr>\n    <tr>\n      <td>6560</td>\n      <td>0.029400</td>\n    </tr>\n    <tr>\n      <td>6570</td>\n      <td>0.024000</td>\n    </tr>\n    <tr>\n      <td>6580</td>\n      <td>0.021000</td>\n    </tr>\n    <tr>\n      <td>6590</td>\n      <td>0.023300</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>0.029900</td>\n    </tr>\n    <tr>\n      <td>6610</td>\n      <td>0.024900</td>\n    </tr>\n    <tr>\n      <td>6620</td>\n      <td>0.021700</td>\n    </tr>\n    <tr>\n      <td>6630</td>\n      <td>0.019000</td>\n    </tr>\n    <tr>\n      <td>6640</td>\n      <td>0.031900</td>\n    </tr>\n    <tr>\n      <td>6650</td>\n      <td>0.019300</td>\n    </tr>\n    <tr>\n      <td>6660</td>\n      <td>0.033800</td>\n    </tr>\n    <tr>\n      <td>6670</td>\n      <td>0.041500</td>\n    </tr>\n    <tr>\n      <td>6680</td>\n      <td>0.024200</td>\n    </tr>\n    <tr>\n      <td>6690</td>\n      <td>0.030400</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>0.021000</td>\n    </tr>\n    <tr>\n      <td>6710</td>\n      <td>0.019500</td>\n    </tr>\n    <tr>\n      <td>6720</td>\n      <td>0.017200</td>\n    </tr>\n    <tr>\n      <td>6730</td>\n      <td>0.015900</td>\n    </tr>\n    <tr>\n      <td>6740</td>\n      <td>0.018500</td>\n    </tr>\n    <tr>\n      <td>6750</td>\n      <td>0.016700</td>\n    </tr>\n    <tr>\n      <td>6760</td>\n      <td>0.013700</td>\n    </tr>\n    <tr>\n      <td>6770</td>\n      <td>0.016700</td>\n    </tr>\n    <tr>\n      <td>6780</td>\n      <td>0.010000</td>\n    </tr>\n    <tr>\n      <td>6790</td>\n      <td>0.031100</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>0.043100</td>\n    </tr>\n    <tr>\n      <td>6810</td>\n      <td>0.021100</td>\n    </tr>\n    <tr>\n      <td>6820</td>\n      <td>0.022100</td>\n    </tr>\n    <tr>\n      <td>6830</td>\n      <td>0.053400</td>\n    </tr>\n    <tr>\n      <td>6840</td>\n      <td>0.017700</td>\n    </tr>\n    <tr>\n      <td>6850</td>\n      <td>0.029200</td>\n    </tr>\n    <tr>\n      <td>6860</td>\n      <td>0.023700</td>\n    </tr>\n    <tr>\n      <td>6870</td>\n      <td>0.024700</td>\n    </tr>\n    <tr>\n      <td>6880</td>\n      <td>0.023700</td>\n    </tr>\n    <tr>\n      <td>6890</td>\n      <td>0.015300</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>0.016000</td>\n    </tr>\n    <tr>\n      <td>6910</td>\n      <td>0.014200</td>\n    </tr>\n    <tr>\n      <td>6920</td>\n      <td>0.017200</td>\n    </tr>\n    <tr>\n      <td>6930</td>\n      <td>0.018600</td>\n    </tr>\n    <tr>\n      <td>6940</td>\n      <td>0.018300</td>\n    </tr>\n    <tr>\n      <td>6950</td>\n      <td>0.018100</td>\n    </tr>\n    <tr>\n      <td>6960</td>\n      <td>0.024200</td>\n    </tr>\n    <tr>\n      <td>6970</td>\n      <td>0.020900</td>\n    </tr>\n    <tr>\n      <td>6980</td>\n      <td>0.039400</td>\n    </tr>\n    <tr>\n      <td>6990</td>\n      <td>0.049400</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.011500</td>\n    </tr>\n    <tr>\n      <td>7010</td>\n      <td>0.017900</td>\n    </tr>\n    <tr>\n      <td>7020</td>\n      <td>0.026500</td>\n    </tr>\n    <tr>\n      <td>7030</td>\n      <td>0.026600</td>\n    </tr>\n    <tr>\n      <td>7040</td>\n      <td>0.025100</td>\n    </tr>\n    <tr>\n      <td>7050</td>\n      <td>0.017400</td>\n    </tr>\n    <tr>\n      <td>7060</td>\n      <td>0.018900</td>\n    </tr>\n    <tr>\n      <td>7070</td>\n      <td>0.019100</td>\n    </tr>\n    <tr>\n      <td>7080</td>\n      <td>0.011400</td>\n    </tr>\n    <tr>\n      <td>7090</td>\n      <td>0.010300</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>0.021300</td>\n    </tr>\n    <tr>\n      <td>7110</td>\n      <td>0.029900</td>\n    </tr>\n    <tr>\n      <td>7120</td>\n      <td>0.019400</td>\n    </tr>\n    <tr>\n      <td>7130</td>\n      <td>0.021700</td>\n    </tr>\n    <tr>\n      <td>7140</td>\n      <td>0.026600</td>\n    </tr>\n    <tr>\n      <td>7150</td>\n      <td>0.019400</td>\n    </tr>\n    <tr>\n      <td>7160</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>7170</td>\n      <td>0.007000</td>\n    </tr>\n    <tr>\n      <td>7180</td>\n      <td>0.030900</td>\n    </tr>\n    <tr>\n      <td>7190</td>\n      <td>0.015700</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>0.033800</td>\n    </tr>\n    <tr>\n      <td>7210</td>\n      <td>0.019200</td>\n    </tr>\n    <tr>\n      <td>7220</td>\n      <td>0.018300</td>\n    </tr>\n    <tr>\n      <td>7230</td>\n      <td>0.010900</td>\n    </tr>\n    <tr>\n      <td>7240</td>\n      <td>0.030100</td>\n    </tr>\n    <tr>\n      <td>7250</td>\n      <td>0.014500</td>\n    </tr>\n    <tr>\n      <td>7260</td>\n      <td>0.029900</td>\n    </tr>\n    <tr>\n      <td>7270</td>\n      <td>0.015600</td>\n    </tr>\n    <tr>\n      <td>7280</td>\n      <td>0.011100</td>\n    </tr>\n    <tr>\n      <td>7290</td>\n      <td>0.012700</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>0.019000</td>\n    </tr>\n    <tr>\n      <td>7310</td>\n      <td>0.018700</td>\n    </tr>\n    <tr>\n      <td>7320</td>\n      <td>0.017500</td>\n    </tr>\n    <tr>\n      <td>7330</td>\n      <td>0.014700</td>\n    </tr>\n    <tr>\n      <td>7340</td>\n      <td>0.016200</td>\n    </tr>\n    <tr>\n      <td>7350</td>\n      <td>0.038900</td>\n    </tr>\n    <tr>\n      <td>7360</td>\n      <td>0.024100</td>\n    </tr>\n    <tr>\n      <td>7370</td>\n      <td>0.018700</td>\n    </tr>\n    <tr>\n      <td>7380</td>\n      <td>0.025900</td>\n    </tr>\n    <tr>\n      <td>7390</td>\n      <td>0.025400</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>0.023000</td>\n    </tr>\n    <tr>\n      <td>7410</td>\n      <td>0.026400</td>\n    </tr>\n    <tr>\n      <td>7420</td>\n      <td>0.026400</td>\n    </tr>\n    <tr>\n      <td>7430</td>\n      <td>0.015000</td>\n    </tr>\n    <tr>\n      <td>7440</td>\n      <td>0.011400</td>\n    </tr>\n    <tr>\n      <td>7450</td>\n      <td>0.020500</td>\n    </tr>\n    <tr>\n      <td>7460</td>\n      <td>0.014800</td>\n    </tr>\n    <tr>\n      <td>7470</td>\n      <td>0.009200</td>\n    </tr>\n    <tr>\n      <td>7480</td>\n      <td>0.037200</td>\n    </tr>\n    <tr>\n      <td>7490</td>\n      <td>0.021900</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.035400</td>\n    </tr>\n    <tr>\n      <td>7510</td>\n      <td>0.032800</td>\n    </tr>\n    <tr>\n      <td>7520</td>\n      <td>0.016000</td>\n    </tr>\n    <tr>\n      <td>7530</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>7540</td>\n      <td>0.041800</td>\n    </tr>\n    <tr>\n      <td>7550</td>\n      <td>0.011800</td>\n    </tr>\n    <tr>\n      <td>7560</td>\n      <td>0.027800</td>\n    </tr>\n    <tr>\n      <td>7570</td>\n      <td>0.024300</td>\n    </tr>\n    <tr>\n      <td>7580</td>\n      <td>0.023800</td>\n    </tr>\n    <tr>\n      <td>7590</td>\n      <td>0.024300</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>0.024100</td>\n    </tr>\n    <tr>\n      <td>7610</td>\n      <td>0.014200</td>\n    </tr>\n    <tr>\n      <td>7620</td>\n      <td>0.014100</td>\n    </tr>\n    <tr>\n      <td>7630</td>\n      <td>0.023100</td>\n    </tr>\n    <tr>\n      <td>7640</td>\n      <td>0.020500</td>\n    </tr>\n    <tr>\n      <td>7650</td>\n      <td>0.022800</td>\n    </tr>\n    <tr>\n      <td>7660</td>\n      <td>0.013600</td>\n    </tr>\n    <tr>\n      <td>7670</td>\n      <td>0.026100</td>\n    </tr>\n    <tr>\n      <td>7680</td>\n      <td>0.025700</td>\n    </tr>\n    <tr>\n      <td>7690</td>\n      <td>0.010600</td>\n    </tr>\n    <tr>\n      <td>7700</td>\n      <td>0.019800</td>\n    </tr>\n    <tr>\n      <td>7710</td>\n      <td>0.023400</td>\n    </tr>\n    <tr>\n      <td>7720</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>7730</td>\n      <td>0.021200</td>\n    </tr>\n    <tr>\n      <td>7740</td>\n      <td>0.016100</td>\n    </tr>\n    <tr>\n      <td>7750</td>\n      <td>0.031900</td>\n    </tr>\n    <tr>\n      <td>7760</td>\n      <td>0.016600</td>\n    </tr>\n    <tr>\n      <td>7770</td>\n      <td>0.025400</td>\n    </tr>\n    <tr>\n      <td>7780</td>\n      <td>0.010600</td>\n    </tr>\n    <tr>\n      <td>7790</td>\n      <td>0.022500</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>0.020000</td>\n    </tr>\n    <tr>\n      <td>7810</td>\n      <td>0.025700</td>\n    </tr>\n    <tr>\n      <td>7820</td>\n      <td>0.027000</td>\n    </tr>\n    <tr>\n      <td>7830</td>\n      <td>0.015400</td>\n    </tr>\n    <tr>\n      <td>7840</td>\n      <td>0.020700</td>\n    </tr>\n    <tr>\n      <td>7850</td>\n      <td>0.007900</td>\n    </tr>\n    <tr>\n      <td>7860</td>\n      <td>0.014400</td>\n    </tr>\n    <tr>\n      <td>7870</td>\n      <td>0.009100</td>\n    </tr>\n    <tr>\n      <td>7880</td>\n      <td>0.020900</td>\n    </tr>\n    <tr>\n      <td>7890</td>\n      <td>0.024200</td>\n    </tr>\n    <tr>\n      <td>7900</td>\n      <td>0.012100</td>\n    </tr>\n    <tr>\n      <td>7910</td>\n      <td>0.019200</td>\n    </tr>\n    <tr>\n      <td>7920</td>\n      <td>0.025600</td>\n    </tr>\n    <tr>\n      <td>7930</td>\n      <td>0.012800</td>\n    </tr>\n    <tr>\n      <td>7940</td>\n      <td>0.031100</td>\n    </tr>\n    <tr>\n      <td>7950</td>\n      <td>0.009700</td>\n    </tr>\n    <tr>\n      <td>7960</td>\n      <td>0.018800</td>\n    </tr>\n    <tr>\n      <td>7970</td>\n      <td>0.027200</td>\n    </tr>\n    <tr>\n      <td>7980</td>\n      <td>0.011900</td>\n    </tr>\n    <tr>\n      <td>7990</td>\n      <td>0.046500</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.032700</td>\n    </tr>\n    <tr>\n      <td>8010</td>\n      <td>0.021300</td>\n    </tr>\n    <tr>\n      <td>8020</td>\n      <td>0.025900</td>\n    </tr>\n    <tr>\n      <td>8030</td>\n      <td>0.034100</td>\n    </tr>\n    <tr>\n      <td>8040</td>\n      <td>0.021200</td>\n    </tr>\n    <tr>\n      <td>8050</td>\n      <td>0.019300</td>\n    </tr>\n    <tr>\n      <td>8060</td>\n      <td>0.026400</td>\n    </tr>\n    <tr>\n      <td>8070</td>\n      <td>0.015000</td>\n    </tr>\n    <tr>\n      <td>8080</td>\n      <td>0.034900</td>\n    </tr>\n    <tr>\n      <td>8090</td>\n      <td>0.016400</td>\n    </tr>\n    <tr>\n      <td>8100</td>\n      <td>0.013000</td>\n    </tr>\n    <tr>\n      <td>8110</td>\n      <td>0.014400</td>\n    </tr>\n    <tr>\n      <td>8120</td>\n      <td>0.016400</td>\n    </tr>\n    <tr>\n      <td>8130</td>\n      <td>0.023600</td>\n    </tr>\n    <tr>\n      <td>8140</td>\n      <td>0.018800</td>\n    </tr>\n    <tr>\n      <td>8150</td>\n      <td>0.024500</td>\n    </tr>\n    <tr>\n      <td>8160</td>\n      <td>0.018200</td>\n    </tr>\n    <tr>\n      <td>8170</td>\n      <td>0.019500</td>\n    </tr>\n    <tr>\n      <td>8180</td>\n      <td>0.029300</td>\n    </tr>\n    <tr>\n      <td>8190</td>\n      <td>0.029500</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>0.035800</td>\n    </tr>\n    <tr>\n      <td>8210</td>\n      <td>0.022700</td>\n    </tr>\n    <tr>\n      <td>8220</td>\n      <td>0.014900</td>\n    </tr>\n    <tr>\n      <td>8230</td>\n      <td>0.017500</td>\n    </tr>\n    <tr>\n      <td>8240</td>\n      <td>0.018100</td>\n    </tr>\n    <tr>\n      <td>8250</td>\n      <td>0.021400</td>\n    </tr>\n    <tr>\n      <td>8260</td>\n      <td>0.028700</td>\n    </tr>\n    <tr>\n      <td>8270</td>\n      <td>0.016300</td>\n    </tr>\n    <tr>\n      <td>8280</td>\n      <td>0.014700</td>\n    </tr>\n    <tr>\n      <td>8290</td>\n      <td>0.029200</td>\n    </tr>\n    <tr>\n      <td>8300</td>\n      <td>0.029500</td>\n    </tr>\n    <tr>\n      <td>8310</td>\n      <td>0.015300</td>\n    </tr>\n    <tr>\n      <td>8320</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>8330</td>\n      <td>0.009800</td>\n    </tr>\n    <tr>\n      <td>8340</td>\n      <td>0.026200</td>\n    </tr>\n    <tr>\n      <td>8350</td>\n      <td>0.023800</td>\n    </tr>\n    <tr>\n      <td>8360</td>\n      <td>0.012900</td>\n    </tr>\n    <tr>\n      <td>8370</td>\n      <td>0.039100</td>\n    </tr>\n    <tr>\n      <td>8380</td>\n      <td>0.024800</td>\n    </tr>\n    <tr>\n      <td>8390</td>\n      <td>0.012500</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>0.021500</td>\n    </tr>\n    <tr>\n      <td>8410</td>\n      <td>0.019900</td>\n    </tr>\n    <tr>\n      <td>8420</td>\n      <td>0.018900</td>\n    </tr>\n    <tr>\n      <td>8430</td>\n      <td>0.010500</td>\n    </tr>\n    <tr>\n      <td>8440</td>\n      <td>0.019000</td>\n    </tr>\n    <tr>\n      <td>8450</td>\n      <td>0.021300</td>\n    </tr>\n    <tr>\n      <td>8460</td>\n      <td>0.007300</td>\n    </tr>\n    <tr>\n      <td>8470</td>\n      <td>0.021600</td>\n    </tr>\n    <tr>\n      <td>8480</td>\n      <td>0.027500</td>\n    </tr>\n    <tr>\n      <td>8490</td>\n      <td>0.018100</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.018200</td>\n    </tr>\n    <tr>\n      <td>8510</td>\n      <td>0.012000</td>\n    </tr>\n    <tr>\n      <td>8520</td>\n      <td>0.023600</td>\n    </tr>\n    <tr>\n      <td>8530</td>\n      <td>0.020600</td>\n    </tr>\n    <tr>\n      <td>8540</td>\n      <td>0.022500</td>\n    </tr>\n    <tr>\n      <td>8550</td>\n      <td>0.016900</td>\n    </tr>\n    <tr>\n      <td>8560</td>\n      <td>0.017600</td>\n    </tr>\n    <tr>\n      <td>8570</td>\n      <td>0.017900</td>\n    </tr>\n    <tr>\n      <td>8580</td>\n      <td>0.017500</td>\n    </tr>\n    <tr>\n      <td>8590</td>\n      <td>0.019000</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>0.023200</td>\n    </tr>\n    <tr>\n      <td>8610</td>\n      <td>0.021500</td>\n    </tr>\n    <tr>\n      <td>8620</td>\n      <td>0.023700</td>\n    </tr>\n    <tr>\n      <td>8630</td>\n      <td>0.029700</td>\n    </tr>\n    <tr>\n      <td>8640</td>\n      <td>0.026600</td>\n    </tr>\n    <tr>\n      <td>8650</td>\n      <td>0.020400</td>\n    </tr>\n    <tr>\n      <td>8660</td>\n      <td>0.015500</td>\n    </tr>\n    <tr>\n      <td>8670</td>\n      <td>0.007400</td>\n    </tr>\n    <tr>\n      <td>8680</td>\n      <td>0.004700</td>\n    </tr>\n    <tr>\n      <td>8690</td>\n      <td>0.026000</td>\n    </tr>\n    <tr>\n      <td>8700</td>\n      <td>0.025900</td>\n    </tr>\n    <tr>\n      <td>8710</td>\n      <td>0.027600</td>\n    </tr>\n    <tr>\n      <td>8720</td>\n      <td>0.014700</td>\n    </tr>\n    <tr>\n      <td>8730</td>\n      <td>0.025200</td>\n    </tr>\n    <tr>\n      <td>8740</td>\n      <td>0.019600</td>\n    </tr>\n    <tr>\n      <td>8750</td>\n      <td>0.019100</td>\n    </tr>\n    <tr>\n      <td>8760</td>\n      <td>0.023300</td>\n    </tr>\n    <tr>\n      <td>8770</td>\n      <td>0.015600</td>\n    </tr>\n    <tr>\n      <td>8780</td>\n      <td>0.023900</td>\n    </tr>\n    <tr>\n      <td>8790</td>\n      <td>0.020800</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>0.018600</td>\n    </tr>\n    <tr>\n      <td>8810</td>\n      <td>0.020000</td>\n    </tr>\n    <tr>\n      <td>8820</td>\n      <td>0.018400</td>\n    </tr>\n    <tr>\n      <td>8830</td>\n      <td>0.012100</td>\n    </tr>\n    <tr>\n      <td>8840</td>\n      <td>0.030600</td>\n    </tr>\n    <tr>\n      <td>8850</td>\n      <td>0.020300</td>\n    </tr>\n    <tr>\n      <td>8860</td>\n      <td>0.027200</td>\n    </tr>\n    <tr>\n      <td>8870</td>\n      <td>0.026500</td>\n    </tr>\n    <tr>\n      <td>8880</td>\n      <td>0.026100</td>\n    </tr>\n    <tr>\n      <td>8890</td>\n      <td>0.021700</td>\n    </tr>\n    <tr>\n      <td>8900</td>\n      <td>0.031900</td>\n    </tr>\n    <tr>\n      <td>8910</td>\n      <td>0.014600</td>\n    </tr>\n    <tr>\n      <td>8920</td>\n      <td>0.021000</td>\n    </tr>\n    <tr>\n      <td>8930</td>\n      <td>0.017700</td>\n    </tr>\n    <tr>\n      <td>8940</td>\n      <td>0.016000</td>\n    </tr>\n    <tr>\n      <td>8950</td>\n      <td>0.017900</td>\n    </tr>\n    <tr>\n      <td>8960</td>\n      <td>0.014900</td>\n    </tr>\n    <tr>\n      <td>8970</td>\n      <td>0.014400</td>\n    </tr>\n    <tr>\n      <td>8980</td>\n      <td>0.008600</td>\n    </tr>\n    <tr>\n      <td>8990</td>\n      <td>0.024000</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.019700</td>\n    </tr>\n    <tr>\n      <td>9010</td>\n      <td>0.015700</td>\n    </tr>\n    <tr>\n      <td>9020</td>\n      <td>0.017500</td>\n    </tr>\n    <tr>\n      <td>9030</td>\n      <td>0.017200</td>\n    </tr>\n    <tr>\n      <td>9040</td>\n      <td>0.023600</td>\n    </tr>\n    <tr>\n      <td>9050</td>\n      <td>0.018500</td>\n    </tr>\n    <tr>\n      <td>9060</td>\n      <td>0.018600</td>\n    </tr>\n    <tr>\n      <td>9070</td>\n      <td>0.015100</td>\n    </tr>\n    <tr>\n      <td>9080</td>\n      <td>0.011900</td>\n    </tr>\n    <tr>\n      <td>9090</td>\n      <td>0.029500</td>\n    </tr>\n    <tr>\n      <td>9100</td>\n      <td>0.018900</td>\n    </tr>\n    <tr>\n      <td>9110</td>\n      <td>0.014900</td>\n    </tr>\n    <tr>\n      <td>9120</td>\n      <td>0.032300</td>\n    </tr>\n    <tr>\n      <td>9130</td>\n      <td>0.014400</td>\n    </tr>\n    <tr>\n      <td>9140</td>\n      <td>0.009500</td>\n    </tr>\n    <tr>\n      <td>9150</td>\n      <td>0.020800</td>\n    </tr>\n    <tr>\n      <td>9160</td>\n      <td>0.020500</td>\n    </tr>\n    <tr>\n      <td>9170</td>\n      <td>0.031800</td>\n    </tr>\n    <tr>\n      <td>9180</td>\n      <td>0.019500</td>\n    </tr>\n    <tr>\n      <td>9190</td>\n      <td>0.021400</td>\n    </tr>\n    <tr>\n      <td>9200</td>\n      <td>0.020600</td>\n    </tr>\n    <tr>\n      <td>9210</td>\n      <td>0.019400</td>\n    </tr>\n    <tr>\n      <td>9220</td>\n      <td>0.011600</td>\n    </tr>\n    <tr>\n      <td>9230</td>\n      <td>0.020500</td>\n    </tr>\n    <tr>\n      <td>9240</td>\n      <td>0.029700</td>\n    </tr>\n    <tr>\n      <td>9250</td>\n      <td>0.015900</td>\n    </tr>\n    <tr>\n      <td>9260</td>\n      <td>0.013600</td>\n    </tr>\n    <tr>\n      <td>9270</td>\n      <td>0.016400</td>\n    </tr>\n    <tr>\n      <td>9280</td>\n      <td>0.014900</td>\n    </tr>\n    <tr>\n      <td>9290</td>\n      <td>0.020400</td>\n    </tr>\n    <tr>\n      <td>9300</td>\n      <td>0.026300</td>\n    </tr>\n    <tr>\n      <td>9310</td>\n      <td>0.020500</td>\n    </tr>\n    <tr>\n      <td>9320</td>\n      <td>0.024200</td>\n    </tr>\n    <tr>\n      <td>9330</td>\n      <td>0.014700</td>\n    </tr>\n    <tr>\n      <td>9340</td>\n      <td>0.029100</td>\n    </tr>\n    <tr>\n      <td>9350</td>\n      <td>0.016800</td>\n    </tr>\n    <tr>\n      <td>9360</td>\n      <td>0.027300</td>\n    </tr>\n    <tr>\n      <td>9370</td>\n      <td>0.019500</td>\n    </tr>\n    <tr>\n      <td>9380</td>\n      <td>0.015900</td>\n    </tr>\n    <tr>\n      <td>9390</td>\n      <td>0.034500</td>\n    </tr>\n    <tr>\n      <td>9400</td>\n      <td>0.018300</td>\n    </tr>\n    <tr>\n      <td>9410</td>\n      <td>0.018400</td>\n    </tr>\n    <tr>\n      <td>9420</td>\n      <td>0.012500</td>\n    </tr>\n    <tr>\n      <td>9430</td>\n      <td>0.022300</td>\n    </tr>\n    <tr>\n      <td>9440</td>\n      <td>0.017700</td>\n    </tr>\n    <tr>\n      <td>9450</td>\n      <td>0.017200</td>\n    </tr>\n    <tr>\n      <td>9460</td>\n      <td>0.033800</td>\n    </tr>\n    <tr>\n      <td>9470</td>\n      <td>0.013300</td>\n    </tr>\n    <tr>\n      <td>9480</td>\n      <td>0.015100</td>\n    </tr>\n    <tr>\n      <td>9490</td>\n      <td>0.017300</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.014900</td>\n    </tr>\n    <tr>\n      <td>9510</td>\n      <td>0.023900</td>\n    </tr>\n    <tr>\n      <td>9520</td>\n      <td>0.021500</td>\n    </tr>\n    <tr>\n      <td>9530</td>\n      <td>0.020800</td>\n    </tr>\n    <tr>\n      <td>9540</td>\n      <td>0.022600</td>\n    </tr>\n    <tr>\n      <td>9550</td>\n      <td>0.021100</td>\n    </tr>\n    <tr>\n      <td>9560</td>\n      <td>0.013400</td>\n    </tr>\n    <tr>\n      <td>9570</td>\n      <td>0.011800</td>\n    </tr>\n    <tr>\n      <td>9580</td>\n      <td>0.017500</td>\n    </tr>\n    <tr>\n      <td>9590</td>\n      <td>0.021700</td>\n    </tr>\n    <tr>\n      <td>9600</td>\n      <td>0.013600</td>\n    </tr>\n    <tr>\n      <td>9610</td>\n      <td>0.028600</td>\n    </tr>\n    <tr>\n      <td>9620</td>\n      <td>0.031600</td>\n    </tr>\n    <tr>\n      <td>9630</td>\n      <td>0.003200</td>\n    </tr>\n    <tr>\n      <td>9640</td>\n      <td>0.031600</td>\n    </tr>\n    <tr>\n      <td>9650</td>\n      <td>0.007900</td>\n    </tr>\n    <tr>\n      <td>9660</td>\n      <td>0.020600</td>\n    </tr>\n    <tr>\n      <td>9670</td>\n      <td>0.017300</td>\n    </tr>\n    <tr>\n      <td>9680</td>\n      <td>0.020400</td>\n    </tr>\n    <tr>\n      <td>9690</td>\n      <td>0.024200</td>\n    </tr>\n    <tr>\n      <td>9700</td>\n      <td>0.025600</td>\n    </tr>\n    <tr>\n      <td>9710</td>\n      <td>0.017100</td>\n    </tr>\n    <tr>\n      <td>9720</td>\n      <td>0.025200</td>\n    </tr>\n    <tr>\n      <td>9730</td>\n      <td>0.007000</td>\n    </tr>\n    <tr>\n      <td>9740</td>\n      <td>0.024800</td>\n    </tr>\n    <tr>\n      <td>9750</td>\n      <td>0.017800</td>\n    </tr>\n    <tr>\n      <td>9760</td>\n      <td>0.020900</td>\n    </tr>\n    <tr>\n      <td>9770</td>\n      <td>0.018100</td>\n    </tr>\n    <tr>\n      <td>9780</td>\n      <td>0.017300</td>\n    </tr>\n    <tr>\n      <td>9790</td>\n      <td>0.028700</td>\n    </tr>\n    <tr>\n      <td>9800</td>\n      <td>0.024900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./deberta_results/checkpoint-500\nConfiguration saved in ./deberta_results/checkpoint-500/config.json\nModel weights saved in ./deberta_results/checkpoint-500/model.safetensors\ntokenizer config file saved in ./deberta_results/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./deberta_results/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to ./deberta_results/checkpoint-1000\nConfiguration saved in ./deberta_results/checkpoint-1000/config.json\nModel weights saved in ./deberta_results/checkpoint-1000/model.safetensors\ntokenizer config file saved in ./deberta_results/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./deberta_results/checkpoint-1000/special_tokens_map.json\nDeleting older checkpoint [deberta_results/checkpoint-500] due to args.save_total_limit\nSaving model checkpoint to ./deberta_results/checkpoint-1500\nConfiguration saved in ./deberta_results/checkpoint-1500/config.json\nModel weights saved in ./deberta_results/checkpoint-1500/model.safetensors\ntokenizer config file saved in ./deberta_results/checkpoint-1500/tokenizer_config.json\nSpecial tokens file saved in ./deberta_results/checkpoint-1500/special_tokens_map.json\nDeleting older checkpoint [deberta_results/checkpoint-1000] due to args.save_total_limit\nSaving model checkpoint to ./deberta_results/checkpoint-2000\nConfiguration saved in ./deberta_results/checkpoint-2000/config.json\nModel weights saved in ./deberta_results/checkpoint-2000/model.safetensors\ntokenizer config file saved in ./deberta_results/checkpoint-2000/tokenizer_config.json\nSpecial tokens file saved in ./deberta_results/checkpoint-2000/special_tokens_map.json\nDeleting older checkpoint [deberta_results/checkpoint-1500] due to args.save_total_limit\nSaving model checkpoint to ./deberta_results/checkpoint-2500\nConfiguration saved in ./deberta_results/checkpoint-2500/config.json\nModel weights saved in ./deberta_results/checkpoint-2500/model.safetensors\ntokenizer config file saved in ./deberta_results/checkpoint-2500/tokenizer_config.json\nSpecial tokens file saved in ./deberta_results/checkpoint-2500/special_tokens_map.json\nDeleting older checkpoint [deberta_results/checkpoint-2000] due to args.save_total_limit\nSaving model checkpoint to ./deberta_results/checkpoint-3000\nConfiguration saved in ./deberta_results/checkpoint-3000/config.json\nModel weights saved in ./deberta_results/checkpoint-3000/model.safetensors\ntokenizer config file saved in ./deberta_results/checkpoint-3000/tokenizer_config.json\nSpecial tokens file saved in ./deberta_results/checkpoint-3000/special_tokens_map.json\nDeleting older checkpoint [deberta_results/checkpoint-2500] due to args.save_total_limit\nSaving model checkpoint to ./deberta_results/checkpoint-3500\nConfiguration saved in ./deberta_results/checkpoint-3500/config.json\nModel weights saved in ./deberta_results/checkpoint-3500/model.safetensors\ntokenizer config file saved in ./deberta_results/checkpoint-3500/tokenizer_config.json\nSpecial tokens file saved in ./deberta_results/checkpoint-3500/special_tokens_map.json\nDeleting older checkpoint [deberta_results/checkpoint-3000] due to args.save_total_limit\nSaving model checkpoint to ./deberta_results/checkpoint-4000\nConfiguration saved in ./deberta_results/checkpoint-4000/config.json\nModel weights saved in ./deberta_results/checkpoint-4000/model.safetensors\ntokenizer config file saved in ./deberta_results/checkpoint-4000/tokenizer_config.json\nSpecial tokens file saved in ./deberta_results/checkpoint-4000/special_tokens_map.json\nDeleting older checkpoint [deberta_results/checkpoint-3500] due to args.save_total_limit\nSaving model checkpoint to ./deberta_results/checkpoint-4500\nConfiguration saved in ./deberta_results/checkpoint-4500/config.json\nModel weights saved in ./deberta_results/checkpoint-4500/model.safetensors\ntokenizer config file saved in ./deberta_results/checkpoint-4500/tokenizer_config.json\nSpecial tokens file saved in ./deberta_results/checkpoint-4500/special_tokens_map.json\nDeleting older checkpoint [deberta_results/checkpoint-4000] due to args.save_total_limit\nSaving model checkpoint to ./deberta_results/checkpoint-5000\nConfiguration saved in ./deberta_results/checkpoint-5000/config.json\nModel weights saved in ./deberta_results/checkpoint-5000/model.safetensors\ntokenizer config file saved in ./deberta_results/checkpoint-5000/tokenizer_config.json\nSpecial tokens file saved in ./deberta_results/checkpoint-5000/special_tokens_map.json\nDeleting older checkpoint [deberta_results/checkpoint-4500] due to args.save_total_limit\nSaving model checkpoint to ./deberta_results/checkpoint-5500\nConfiguration saved in ./deberta_results/checkpoint-5500/config.json\nModel weights saved in ./deberta_results/checkpoint-5500/model.safetensors\ntokenizer config file saved in ./deberta_results/checkpoint-5500/tokenizer_config.json\nSpecial tokens file saved in ./deberta_results/checkpoint-5500/special_tokens_map.json\nDeleting older checkpoint [deberta_results/checkpoint-5000] due to args.save_total_limit\nSaving model checkpoint to ./deberta_results/checkpoint-6000\nConfiguration saved in ./deberta_results/checkpoint-6000/config.json\nModel weights saved in ./deberta_results/checkpoint-6000/model.safetensors\ntokenizer config file saved in ./deberta_results/checkpoint-6000/tokenizer_config.json\nSpecial tokens file saved in ./deberta_results/checkpoint-6000/special_tokens_map.json\nDeleting older checkpoint [deberta_results/checkpoint-5500] due to args.save_total_limit\nSaving model checkpoint to ./deberta_results/checkpoint-6500\nConfiguration saved in ./deberta_results/checkpoint-6500/config.json\nModel weights saved in ./deberta_results/checkpoint-6500/model.safetensors\ntokenizer config file saved in ./deberta_results/checkpoint-6500/tokenizer_config.json\nSpecial tokens file saved in ./deberta_results/checkpoint-6500/special_tokens_map.json\nDeleting older checkpoint [deberta_results/checkpoint-6000] due to args.save_total_limit\nSaving model checkpoint to ./deberta_results/checkpoint-7000\nConfiguration saved in ./deberta_results/checkpoint-7000/config.json\nModel weights saved in ./deberta_results/checkpoint-7000/model.safetensors\ntokenizer config file saved in ./deberta_results/checkpoint-7000/tokenizer_config.json\nSpecial tokens file saved in ./deberta_results/checkpoint-7000/special_tokens_map.json\nDeleting older checkpoint [deberta_results/checkpoint-6500] due to args.save_total_limit\nSaving model checkpoint to ./deberta_results/checkpoint-7500\nConfiguration saved in ./deberta_results/checkpoint-7500/config.json\nModel weights saved in ./deberta_results/checkpoint-7500/model.safetensors\ntokenizer config file saved in ./deberta_results/checkpoint-7500/tokenizer_config.json\nSpecial tokens file saved in ./deberta_results/checkpoint-7500/special_tokens_map.json\nDeleting older checkpoint [deberta_results/checkpoint-7000] due to args.save_total_limit\nSaving model checkpoint to ./deberta_results/checkpoint-8000\nConfiguration saved in ./deberta_results/checkpoint-8000/config.json\nModel weights saved in ./deberta_results/checkpoint-8000/model.safetensors\ntokenizer config file saved in ./deberta_results/checkpoint-8000/tokenizer_config.json\nSpecial tokens file saved in ./deberta_results/checkpoint-8000/special_tokens_map.json\nDeleting older checkpoint [deberta_results/checkpoint-7500] due to args.save_total_limit\nSaving model checkpoint to ./deberta_results/checkpoint-8500\nConfiguration saved in ./deberta_results/checkpoint-8500/config.json\nModel weights saved in ./deberta_results/checkpoint-8500/model.safetensors\ntokenizer config file saved in ./deberta_results/checkpoint-8500/tokenizer_config.json\nSpecial tokens file saved in ./deberta_results/checkpoint-8500/special_tokens_map.json\nDeleting older checkpoint [deberta_results/checkpoint-8000] due to args.save_total_limit\nSaving model checkpoint to ./deberta_results/checkpoint-9000\nConfiguration saved in ./deberta_results/checkpoint-9000/config.json\nModel weights saved in ./deberta_results/checkpoint-9000/model.safetensors\ntokenizer config file saved in ./deberta_results/checkpoint-9000/tokenizer_config.json\nSpecial tokens file saved in ./deberta_results/checkpoint-9000/special_tokens_map.json\nDeleting older checkpoint [deberta_results/checkpoint-8500] due to args.save_total_limit\nSaving model checkpoint to ./deberta_results/checkpoint-9500\nConfiguration saved in ./deberta_results/checkpoint-9500/config.json\nModel weights saved in ./deberta_results/checkpoint-9500/model.safetensors\ntokenizer config file saved in ./deberta_results/checkpoint-9500/tokenizer_config.json\nSpecial tokens file saved in ./deberta_results/checkpoint-9500/special_tokens_map.json\nDeleting older checkpoint [deberta_results/checkpoint-9000] due to args.save_total_limit\nSaving model checkpoint to ./deberta_results/checkpoint-9800\nConfiguration saved in ./deberta_results/checkpoint-9800/config.json\nModel weights saved in ./deberta_results/checkpoint-9800/model.safetensors\ntokenizer config file saved in ./deberta_results/checkpoint-9800/tokenizer_config.json\nSpecial tokens file saved in ./deberta_results/checkpoint-9800/special_tokens_map.json\nDeleting older checkpoint [deberta_results/checkpoint-9500] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nThe following columns in the Evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: Label, __index_level_0__, Text. If Label, __index_level_0__, Text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 783\n  Batch size = 32\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25/25 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Test metrics: {'eval_loss': 0.5906355381011963, 'eval_accuracy': 0.9361430395913155, 'eval_precision': 0.9366492703470523, 'eval_recall': 0.9361430395913155, 'eval_f1': 0.9362945898231654, 'eval_runtime': 1.9365, 'eval_samples_per_second': 404.331, 'eval_steps_per_second': 12.91, 'epoch': 50.0}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nimport torch.nn.functional as F\n\n# Path to your trained checkpoint\nmodel_path = \"./deberta_results/checkpoint-9800\"\n\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\n\n# Put model in evaluation mode\nmodel.eval()\n\ndef predict(text):\n    # Tokenize input\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    \n    # Forward pass\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        probs = F.softmax(logits, dim=-1)\n        pred = torch.argmax(probs, dim=-1).item()\n\n    return pred, probs.tolist()\n\n# Example usage\nwhile True:\n    user_input = input(\"✍️ Enter your response (type 'x' to close): \")\n    if user_input.lower() == 'x':\n        print(\"🔒 Closed.\")\n        break\n    \n    label, probabilities = predict(user_input)\n    \n    # Assuming label mapping: 0 = Not Bullying, 1 = Bullying\n    if label == 1:\n        print(f\"🚨 Prediction: Bullying (confidence: {probabilities[0][label]:.2f})\")\n    else:\n        print(f\"✅ Prediction: Not Bullying (confidence: {probabilities[0][label]:.2f})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T02:41:52.145571Z","iopub.execute_input":"2025-09-16T02:41:52.146282Z","iopub.status.idle":"2025-09-16T02:55:24.325217Z","shell.execute_reply.started":"2025-09-16T02:41:52.146224Z","shell.execute_reply":"2025-09-16T02:55:24.324512Z"}},"outputs":[{"name":"stderr","text":"loading file spm.model\nloading file tokenizer.json\nloading file added_tokens.json\nloading file special_tokens_map.json\nloading file tokenizer_config.json\nloading file chat_template.jinja\nloading configuration file ./deberta_results/checkpoint-9800/config.json\nModel config DebertaV2Config {\n  \"architectures\": [\n    \"DebertaV2ForSequenceClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"legacy\": true,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.52.4\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file ./deberta_results/checkpoint-9800/model.safetensors\nAll model checkpoint weights were used when initializing DebertaV2ForSequenceClassification.\n\nAll the weights of DebertaV2ForSequenceClassification were initialized from the model checkpoint at ./deberta_results/checkpoint-9800.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2ForSequenceClassification for predictions without further training.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"✍️ Enter your response (type 'x' to close):  My phone is not working. Can you fix it?\n"},{"name":"stdout","text":"✅ Prediction: Not Bullying (confidence: 0.99)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"✍️ Enter your response (type 'x' to close):  I have my photo in my diary. Do you wanna see it?\n"},{"name":"stdout","text":"🚨 Prediction: Bullying (confidence: 1.00)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"✍️ Enter your response (type 'x' to close):  What's the time?\n"},{"name":"stdout","text":"✅ Prediction: Not Bullying (confidence: 0.91)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"✍️ Enter your response (type 'x' to close):  I'd like to fly in the air.\n"},{"name":"stdout","text":"✅ Prediction: Not Bullying (confidence: 0.97)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"✍️ Enter your response (type 'x' to close):  This place is really beautiful. This is a old school. I am a regular visitor.\n"},{"name":"stdout","text":"✅ Prediction: Not Bullying (confidence: 1.00)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"✍️ Enter your response (type 'x' to close):  This place is really beautiful. This is a brothel. I am a regular visitor.\n"},{"name":"stdout","text":"✅ Prediction: Not Bullying (confidence: 1.00)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"✍️ Enter your response (type 'x' to close):  This is a beautiful hotel. \n"},{"name":"stdout","text":"✅ Prediction: Not Bullying (confidence: 1.00)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"✍️ Enter your response (type 'x' to close):  This is a beautiful hotel. Do you wanna come with me?\n"},{"name":"stdout","text":"✅ Prediction: Not Bullying (confidence: 1.00)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"✍️ Enter your response (type 'x' to close):  This is a beautiful hotel. Do you wanna come with me? I wanna book a room for us and spend a couple of days with you.\n"},{"name":"stdout","text":"✅ Prediction: Not Bullying (confidence: 1.00)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"✍️ Enter your response (type 'x' to close):  Do you wanna fun in the bedroom with me?\n"},{"name":"stdout","text":"🚨 Prediction: Bullying (confidence: 1.00)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"✍️ Enter your response (type 'x' to close):  You have a nice car. When did you buy it?\n"},{"name":"stdout","text":"✅ Prediction: Not Bullying (confidence: 1.00)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"✍️ Enter your response (type 'x' to close):  You have a nice hair style? Where did you make it?\n"},{"name":"stdout","text":"✅ Prediction: Not Bullying (confidence: 1.00)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"✍️ Enter your response (type 'x' to close):  You have a nice hair style. It attracts me.\n"},{"name":"stdout","text":"🚨 Prediction: Bullying (confidence: 1.00)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"✍️ Enter your response (type 'x' to close):  You  have nice cloths. May I know how do you look without them?\n"},{"name":"stdout","text":"✅ Prediction: Not Bullying (confidence: 1.00)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"✍️ Enter your response (type 'x' to close):  You have nice clothes. May I know how do you look without them?\n"},{"name":"stdout","text":"🚨 Prediction: Bullying (confidence: 0.96)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"✍️ Enter your response (type 'x' to close):  You look amazing, but I don't like your face.\n"},{"name":"stdout","text":"🚨 Prediction: Bullying (confidence: 0.90)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"✍️ Enter your response (type 'x' to close):  Do you wanna drink coffee ?\n"},{"name":"stdout","text":"🚨 Prediction: Bullying (confidence: 1.00)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"✍️ Enter your response (type 'x' to close):  X\n"},{"name":"stdout","text":"🔒 Closed.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}